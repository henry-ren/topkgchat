nohup: 忽略输入
Global seed set to 0
Some weights of the model checkpoint at cache/bert were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at cache/bert and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/home/miniconda3/lib/python3.9/site-packages/pytorch_lightning/loops/utilities.py:91: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.
  rank_zero_warn(
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
Missing logger folder: logs_rl/
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name      | Type             | Params
-----------------------------------------------
0 | predictor | KeywordPredictor | 16.6 M
1 | critic    | Critic           | 15.0 M
2 | actor     | ActorCategorical | 635 K 
-----------------------------------------------
31.6 M    Trainable params
0         Non-trainable params
31.6 M    Total params
126.550   Total estimated model params size (MB)
/home/miniconda3/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
Training: 0it [00:00, ?it/s]Epoch 0: : 0it [00:00, ?it/s]Epoch 0: : 0it [04:20, ?it/s, loss=1.33, v_num=0, avg_ep_len=1.500, avg_ep_reward=-.0399, avg_reward=-.00689, loss_actor=0.183]Epoch 0, global step 8: 'avg_ep_reward' reached -0.03992 (best -0.03992), saving model to 'logs_rl/version_0/checkpoints/best.ckpt' as top 1
Epoch 1: : 0it [04:20, ?it/s, loss=1.33, v_num=0, avg_ep_len=1.500, avg_ep_reward=-.0399, avg_reward=-.00689, loss_actor=0.183]Epoch 1: : 0it [08:47, ?it/s, loss=1.4, v_num=0, avg_ep_len=5.970, avg_ep_reward=-.040, avg_reward=-.00867, loss_actor=0.165]  Epoch 1, global step 16: 'avg_ep_reward' was not in top 1
Epoch 2: : 0it [08:47, ?it/s, loss=1.4, v_num=0, avg_ep_len=5.970, avg_ep_reward=-.040, avg_reward=-.00867, loss_actor=0.165]Epoch 2: : 0it [12:56, ?it/s, loss=1.64, v_num=0, avg_ep_len=5.910, avg_ep_reward=0.159, avg_reward=0.0249, loss_actor=0.337]Epoch 2, global step 24: 'avg_ep_reward' reached 0.15917 (best 0.15917), saving model to 'logs_rl/version_0/checkpoints/best.ckpt' as top 1
Epoch 3: : 0it [12:57, ?it/s, loss=1.64, v_num=0, avg_ep_len=5.910, avg_ep_reward=0.159, avg_reward=0.0249, loss_actor=0.337]/home/hyx/yzt/target-guided/code/global_planing.py:33: RuntimeWarning: invalid value encountered in true_divide
  res = num / denom
Epoch 3: : 0it [17:02, ?it/s, loss=1.56, v_num=0, avg_ep_len=5.970, avg_ep_reward=-.0573, avg_reward=-.00161, loss_actor=0.108]Epoch 3, global step 32: 'avg_ep_reward' was not in top 1
Epoch 4: : 0it [17:03, ?it/s, loss=1.56, v_num=0, avg_ep_len=5.970, avg_ep_reward=-.0573, avg_reward=-.00161, loss_actor=0.108]Epoch 4: : 0it [21:13, ?it/s, loss=1.4, v_num=0, avg_ep_len=5.830, avg_ep_reward=0.555, avg_reward=0.0962, loss_actor=0.188]   Epoch 4, global step 40: 'avg_ep_reward' reached 0.55461 (best 0.55461), saving model to 'logs_rl/version_0/checkpoints/best.ckpt' as top 1
Epoch 5: : 0it [21:14, ?it/s, loss=1.4, v_num=0, avg_ep_len=5.830, avg_ep_reward=0.555, avg_reward=0.0962, loss_actor=0.188]Epoch 5: : 0it [25:38, ?it/s, loss=1.48, v_num=0, avg_ep_len=5.720, avg_ep_reward=0.145, avg_reward=0.0248, loss_actor=0.176]Epoch 5, global step 48: 'avg_ep_reward' was not in top 1
Epoch 6: : 0it [25:39, ?it/s, loss=1.48, v_num=0, avg_ep_len=5.720, avg_ep_reward=0.145, avg_reward=0.0248, loss_actor=0.176]Epoch 6: : 0it [29:49, ?it/s, loss=1.76, v_num=0, avg_ep_len=5.880, avg_ep_reward=0.217, avg_reward=0.0293, loss_actor=0.160]Epoch 6, global step 56: 'avg_ep_reward' was not in top 1
Epoch 7: : 0it [29:49, ?it/s, loss=1.76, v_num=0, avg_ep_len=5.880, avg_ep_reward=0.217, avg_reward=0.0293, loss_actor=0.160]Epoch 7: : 0it [34:08, ?it/s, loss=1.77, v_num=0, avg_ep_len=5.850, avg_ep_reward=-.599, avg_reward=-.109, loss_actor=0.391] Epoch 7, global step 64: 'avg_ep_reward' was not in top 1
Epoch 8: : 0it [34:09, ?it/s, loss=1.77, v_num=0, avg_ep_len=5.850, avg_ep_reward=-.599, avg_reward=-.109, loss_actor=0.391]Epoch 8: : 0it [38:34, ?it/s, loss=1.59, v_num=0, avg_ep_len=5.740, avg_ep_reward=-.320, avg_reward=-.0511, loss_actor=0.203]Epoch 8, global step 72: 'avg_ep_reward' was not in top 1
Epoch 9: : 0it [38:35, ?it/s, loss=1.59, v_num=0, avg_ep_len=5.740, avg_ep_reward=-.320, avg_reward=-.0511, loss_actor=0.203]Epoch 9: : 0it [43:18, ?it/s, loss=1.51, v_num=0, avg_ep_len=5.730, avg_ep_reward=0.107, avg_reward=0.013, loss_actor=0.207] Epoch 9, global step 80: 'avg_ep_reward' was not in top 1
Epoch 10: : 0it [43:19, ?it/s, loss=1.51, v_num=0, avg_ep_len=5.730, avg_ep_reward=0.107, avg_reward=0.013, loss_actor=0.207]Epoch 10: : 0it [47:44, ?it/s, loss=1.5, v_num=0, avg_ep_len=5.770, avg_ep_reward=-.0999, avg_reward=-.0179, loss_actor=0.0777]Epoch 10, global step 88: 'avg_ep_reward' was not in top 1
Epoch 11: : 0it [47:45, ?it/s, loss=1.5, v_num=0, avg_ep_len=5.770, avg_ep_reward=-.0999, avg_reward=-.0179, loss_actor=0.0777]----------------------------------------
art
[['picture', 'motion', 'work'], ['work', 'insurance', 'company', 'sound'], ['painting'], ['artistic']]
["doing well thanks off work . i'm an extra for major motion pictures . what do you do ?", 'that sounds exciting . i work for an insurance company .', 'do you do any painting?', 'i do some art.']
----------------------------------------
Epoch 11: : 0it [52:41, ?it/s, loss=1.7, v_num=0, avg_ep_len=5.820, avg_ep_reward=-.179, avg_reward=-.0287, loss_actor=0.147]  Epoch 11, global step 96: 'avg_ep_reward' was not in top 1
Epoch 12: : 0it [52:42, ?it/s, loss=1.7, v_num=0, avg_ep_len=5.820, avg_ep_reward=-.179, avg_reward=-.0287, loss_actor=0.147]Epoch 12: : 0it [58:09, ?it/s, loss=1.81, v_num=0, avg_ep_len=5.730, avg_ep_reward=0.0642, avg_reward=0.0164, loss_actor=0.253]Epoch 12, global step 104: 'avg_ep_reward' was not in top 1
Epoch 13: : 0it [58:10, ?it/s, loss=1.81, v_num=0, avg_ep_len=5.730, avg_ep_reward=0.0642, avg_reward=0.0164, loss_actor=0.253]Epoch 13: : 0it [1:05:35, ?it/s, loss=1.9, v_num=0, avg_ep_len=5.930, avg_ep_reward=-.351, avg_reward=-.0517, loss_actor=0.326]Epoch 13, global step 112: 'avg_ep_reward' was not in top 1
Epoch 14: : 0it [1:05:36, ?it/s, loss=1.9, v_num=0, avg_ep_len=5.930, avg_ep_reward=-.351, avg_reward=-.0517, loss_actor=0.326]Epoch 14: : 0it [1:12:53, ?it/s, loss=1.62, v_num=0, avg_ep_len=5.730, avg_ep_reward=-.0555, avg_reward=-.00382, loss_actor=0.233]Epoch 14, global step 120: 'avg_ep_reward' was not in top 1
Epoch 15: : 0it [1:12:54, ?it/s, loss=1.62, v_num=0, avg_ep_len=5.730, avg_ep_reward=-.0555, avg_reward=-.00382, loss_actor=0.233]Epoch 15: : 0it [1:19:59, ?it/s, loss=1.6, v_num=0, avg_ep_len=5.700, avg_ep_reward=0.739, avg_reward=0.125, loss_actor=0.232]    Epoch 15, global step 128: 'avg_ep_reward' reached 0.73908 (best 0.73908), saving model to 'logs_rl/version_0/checkpoints/best.ckpt' as top 1
Epoch 16: : 0it [1:20:01, ?it/s, loss=1.6, v_num=0, avg_ep_len=5.700, avg_ep_reward=0.739, avg_reward=0.125, loss_actor=0.232]Epoch 16: : 0it [1:25:17, ?it/s, loss=1.57, v_num=0, avg_ep_len=5.670, avg_ep_reward=0.0781, avg_reward=0.0121, loss_actor=0.175]Epoch 16, global step 136: 'avg_ep_reward' was not in top 1
Epoch 17: : 0it [1:25:18, ?it/s, loss=1.57, v_num=0, avg_ep_len=5.670, avg_ep_reward=0.0781, avg_reward=0.0121, loss_actor=0.175]Epoch 17: : 0it [1:29:38, ?it/s, loss=1.55, v_num=0, avg_ep_len=5.830, avg_ep_reward=-.146, avg_reward=-.0207, loss_actor=0.212] Epoch 17, global step 144: 'avg_ep_reward' was not in top 1
Epoch 18: : 0it [1:29:39, ?it/s, loss=1.55, v_num=0, avg_ep_len=5.830, avg_ep_reward=-.146, avg_reward=-.0207, loss_actor=0.212]Epoch 18: : 0it [1:34:43, ?it/s, loss=1.29, v_num=0, avg_ep_len=5.900, avg_ep_reward=-.531, avg_reward=-.0912, loss_actor=0.164]Epoch 18, global step 152: 'avg_ep_reward' was not in top 1
Epoch 19: : 0it [1:34:44, ?it/s, loss=1.29, v_num=0, avg_ep_len=5.900, avg_ep_reward=-.531, avg_reward=-.0912, loss_actor=0.164]Epoch 19: : 0it [1:39:27, ?it/s, loss=1.31, v_num=0, avg_ep_len=5.810, avg_ep_reward=-.407, avg_reward=-.0723, loss_actor=0.137]Epoch 19, global step 160: 'avg_ep_reward' was not in top 1
Epoch 20: : 0it [1:39:28, ?it/s, loss=1.31, v_num=0, avg_ep_len=5.810, avg_ep_reward=-.407, avg_reward=-.0723, loss_actor=0.137]Epoch 20: : 0it [1:44:17, ?it/s, loss=1.52, v_num=0, avg_ep_len=5.960, avg_ep_reward=-.0129, avg_reward=-.000941, loss_actor=0.115]Epoch 20, global step 168: 'avg_ep_reward' was not in top 1
Epoch 21: : 0it [1:44:17, ?it/s, loss=1.52, v_num=0, avg_ep_len=5.960, avg_ep_reward=-.0129, avg_reward=-.000941, loss_actor=0.115]Epoch 21: : 0it [1:49:24, ?it/s, loss=1.52, v_num=0, avg_ep_len=5.830, avg_ep_reward=-.0745, avg_reward=-.0264, loss_actor=0.360]  Epoch 21, global step 176: 'avg_ep_reward' was not in top 1
Epoch 22: : 0it [1:49:25, ?it/s, loss=1.52, v_num=0, avg_ep_len=5.830, avg_ep_reward=-.0745, avg_reward=-.0264, loss_actor=0.360]Epoch 22: : 0it [1:54:06, ?it/s, loss=1.58, v_num=0, avg_ep_len=5.810, avg_ep_reward=-.173, avg_reward=-.0277, loss_actor=0.125] Epoch 22, global step 184: 'avg_ep_reward' was not in top 1
Epoch 23: : 0it [1:54:07, ?it/s, loss=1.58, v_num=0, avg_ep_len=5.810, avg_ep_reward=-.173, avg_reward=-.0277, loss_actor=0.125]Epoch 23: : 0it [1:59:02, ?it/s, loss=1.5, v_num=0, avg_ep_len=5.870, avg_ep_reward=-.0532, avg_reward=-.0129, loss_actor=0.278]Epoch 23, global step 192: 'avg_ep_reward' was not in top 1
Epoch 24: : 0it [1:59:03, ?it/s, loss=1.5, v_num=0, avg_ep_len=5.870, avg_ep_reward=-.0532, avg_reward=-.0129, loss_actor=0.278]Epoch 24: : 0it [2:05:51, ?it/s, loss=1.49, v_num=0, avg_ep_len=5.860, avg_ep_reward=-.332, avg_reward=-.0553, loss_actor=0.349]Epoch 24, global step 200: 'avg_ep_reward' was not in top 1
Epoch 25: : 0it [2:05:52, ?it/s, loss=1.49, v_num=0, avg_ep_len=5.860, avg_ep_reward=-.332, avg_reward=-.0553, loss_actor=0.349]Epoch 25: : 0it [2:12:58, ?it/s, loss=1.42, v_num=0, avg_ep_len=5.770, avg_ep_reward=-.278, avg_reward=-.052, loss_actor=0.168] Epoch 25, global step 208: 'avg_ep_reward' was not in top 1
Epoch 26: : 0it [2:12:59, ?it/s, loss=1.42, v_num=0, avg_ep_len=5.770, avg_ep_reward=-.278, avg_reward=-.052, loss_actor=0.168]Epoch 26: : 0it [2:18:12, ?it/s, loss=1.47, v_num=0, avg_ep_len=5.830, avg_ep_reward=0.257, avg_reward=0.0408, loss_actor=0.155]Epoch 26, global step 216: 'avg_ep_reward' was not in top 1
Epoch 27: : 0it [2:18:13, ?it/s, loss=1.47, v_num=0, avg_ep_len=5.830, avg_ep_reward=0.257, avg_reward=0.0408, loss_actor=0.155]Epoch 27: : 0it [2:24:04, ?it/s, loss=1.73, v_num=0, avg_ep_len=6.000, avg_ep_reward=-.0612, avg_reward=-.0132, loss_actor=0.0874]Epoch 27, global step 224: 'avg_ep_reward' was not in top 1
Epoch 28: : 0it [2:24:05, ?it/s, loss=1.73, v_num=0, avg_ep_len=6.000, avg_ep_reward=-.0612, avg_reward=-.0132, loss_actor=0.0874]Epoch 28: : 0it [2:28:21, ?it/s, loss=1.72, v_num=0, avg_ep_len=5.990, avg_ep_reward=-.348, avg_reward=-.0605, loss_actor=0.269]  Epoch 28, global step 232: 'avg_ep_reward' was not in top 1
Epoch 29: : 0it [2:28:22, ?it/s, loss=1.72, v_num=0, avg_ep_len=5.990, avg_ep_reward=-.348, avg_reward=-.0605, loss_actor=0.269]Epoch 29: : 0it [2:33:13, ?it/s, loss=1.68, v_num=0, avg_ep_len=5.950, avg_ep_reward=-.581, avg_reward=-.0916, loss_actor=0.171]Epoch 29, global step 240: 'avg_ep_reward' was not in top 1
Epoch 30: : 0it [2:33:13, ?it/s, loss=1.68, v_num=0, avg_ep_len=5.950, avg_ep_reward=-.581, avg_reward=-.0916, loss_actor=0.171]Epoch 30: : 0it [2:37:35, ?it/s, loss=1.55, v_num=0, avg_ep_len=5.890, avg_ep_reward=-.403, avg_reward=-.0719, loss_actor=0.130]Epoch 30, global step 248: 'avg_ep_reward' was not in top 1
Epoch 31: : 0it [2:37:36, ?it/s, loss=1.55, v_num=0, avg_ep_len=5.890, avg_ep_reward=-.403, avg_reward=-.0719, loss_actor=0.130]Epoch 31: : 0it [2:41:47, ?it/s, loss=1.67, v_num=0, avg_ep_len=5.930, avg_ep_reward=-.256, avg_reward=-.0414, loss_actor=0.387]Epoch 31, global step 256: 'avg_ep_reward' was not in top 1
Epoch 32: : 0it [2:41:48, ?it/s, loss=1.67, v_num=0, avg_ep_len=5.930, avg_ep_reward=-.256, avg_reward=-.0414, loss_actor=0.387]Epoch 32: : 0it [2:46:28, ?it/s, loss=1.49, v_num=0, avg_ep_len=5.920, avg_ep_reward=-.113, avg_reward=-.0187, loss_actor=0.177]Epoch 32, global step 264: 'avg_ep_reward' was not in top 1
Epoch 33: : 0it [2:46:28, ?it/s, loss=1.49, v_num=0, avg_ep_len=5.920, avg_ep_reward=-.113, avg_reward=-.0187, loss_actor=0.177]Epoch 33: : 0it [2:52:27, ?it/s, loss=1.55, v_num=0, avg_ep_len=5.670, avg_ep_reward=0.0957, avg_reward=0.0176, loss_actor=0.130]Epoch 33, global step 272: 'avg_ep_reward' was not in top 1
Epoch 34: : 0it [2:52:28, ?it/s, loss=1.55, v_num=0, avg_ep_len=5.670, avg_ep_reward=0.0957, avg_reward=0.0176, loss_actor=0.130]